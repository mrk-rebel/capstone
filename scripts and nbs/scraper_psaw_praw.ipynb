{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Scraping\n",
    "This notebook uses PSAW to get archive comments retrieved by Pushshift immediately after they are created. This way, we can get comments before the risk of them being deleted.\n",
    "\n",
    "Using this method, we can get older data, for instance, from 2021. **IMPORTANT**: we need to check that `created_utc` and `retrieved_utc` are close so we can ensure pushshift was retrieving posts immediately after creation at the chosen epoch.\n",
    "\n",
    "Pushshift API [here](https://reddit-api.readthedocs.io/en/latest/) <br/>\n",
    "PRAW API [here](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html)<br/>\n",
    "\n",
    "PMAW documentation [here](https://github.com/mattpodolak/pmaw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### --- to do --- \n",
    "##### before we start scraping:\n",
    "* ~~decide on the epoch~~\n",
    "* ~~finalize dictionary of terms to use in the filter for PSAW~~\n",
    "* ~~decide on how to keep track of all the scraping instances. ~~\n",
    "    * ~~For instance, do we want to use name convention so every time the scraper makes a request and saves new DFs to Google Drive, each DF has a number in the name to reference the time or order in which it was scraped? <br/>Like: `comments_df_202203121400` or `comments_df_001`~~\n",
    "\n",
    "\n",
    "\n",
    "* we will probably encounter problems with <mark>ratelimits</mark> and will have to learn more about it and troubleshoot it\n",
    "\n",
    "\n",
    "### --- hapenning now ---\n",
    "\n",
    "* using pmaw in lieu of psaw because psaw maintainer mentioned pmaw is \"much more actively maintained\" [here](https://github.com/dmarx/psaw/issues/103)\n",
    "    *  trying to see if it works better \n",
    "* starting to scrape the pushshift data first so classification can start\n",
    "* scraping data from every hour of the day for jan2022. if data has less than 100k comments, will include feb2022\n",
    "    * update: retrieved 223 items from the first hour of 2022. We may get enough data scraping either only jan22 or jan22+feb22.<br/>\n",
    "* pushshift data will be included in df named `comments_jan22` (or `comments_monyear` if expanding to other months, each month will have its own file for now)\n",
    "* praw data will populate two other DFs, `submissions_jan22` and `user_jan22`\n",
    "* data from pushshift also populates a reference dictionary that stores `comment_id`, `link_id` and `author` <br/>this is not split into months\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install psaw\n",
    "# pip install praw\n",
    "# pip install pmaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reddit_auth\n",
    "import praw\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=reddit_auth.client_id,\n",
    "    client_secret=reddit_auth.client_secret,\n",
    "    user_agent=reddit_auth.user_agent, \n",
    "    password=reddit_auth.password,\n",
    "    username=reddit_auth.username\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "handler = logging.StreamHandler()\n",
    "handler.setLevel(logging.INFO)\n",
    "\n",
    "logger = logging.getLogger('pmaw')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Definitions\n",
    "\n",
    "* Define features for PSAW\n",
    "* Define search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features for PSAW\n",
    "our_filter = ['author',\n",
    "              'author_flair_type',\n",
    "              'author_fullname',\n",
    "              'author_premium',\n",
    "              'body',\n",
    "              'body_sha1',\n",
    "              'controversiality',\n",
    "              'created_utc',\n",
    "              'distinguished',\n",
    "              'gilded',\n",
    "              'id',\n",
    "              'is_submitter',\n",
    "              'link_id', \n",
    "              'locked',\n",
    "              'parent_id',\n",
    "              'permalink',\n",
    "              'retrieved_utc',\n",
    "              'subreddit',\n",
    "              'subreddit_id',\n",
    "              'subreddit_name_prefixed',\n",
    "              'subreddit_type'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms for our filter\n",
    "hate_terms = pd.read_csv('hate_terms.csv')\n",
    "\n",
    "our_terms = '|'.join(hate_terms.term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Creating master files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comments_jan22.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save reference files with unique identifiers for 'comments', 'submissions', 'users'\n",
    "\n",
    "reference_ids = defaultdict(list)\n",
    "joblib.dump(reference_ids, 'reference_ids.joblib', compress=3)\n",
    "\n",
    "# starting with pushshift first (comments only)\n",
    "# starting with jan22\n",
    "comments_jan22 = pd.DataFrame()\n",
    "joblib.dump(comments_jan22, 'comments_jan22.joblib', compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['users_jan22.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions_jan22 = pd.DataFrame()\n",
    "users_jan22 = pd.DataFrame()\n",
    "joblib.dump(submissions_jan22, 'submissions_jan22.joblib', compress=3)\n",
    "joblib.dump(users_jan22, 'users_jan22.joblib', compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_unique_ids(df):\n",
    "    '''\n",
    "    - Loads reference dictionary file,\n",
    "    - updates it with unique id for comments, submissions, and users, in given df and \n",
    "    - saves updated dictionary as joblib compressed file\n",
    "    '''\n",
    "    import joblib\n",
    "    # load reference dictionary\n",
    "    ref = joblib.load('reference_ids.joblib')\n",
    "    \n",
    "    # update dictionary\n",
    "    ref['comment'].extend(df.id)\n",
    "    ref['submission'].extend(df.link_id)\n",
    "    ref['user'].extend(df.author)\n",
    "    \n",
    "    # save updated reference dictionary\n",
    "    joblib.dump(ref, 'reference_ids.joblib', compress=3)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_retrieved(df, endpoint, month, year):\n",
    "    '''\n",
    "    Loads comments file for specific month, updates it with newly scrapped comments and \n",
    "    saves updated DF as joblib compressed file\n",
    "    - df: df created with newly scraped data\n",
    "    - endpoint: comments | submissions | users\n",
    "    - month: 3 letter lowercap\n",
    "    - year: 2 last digits\n",
    "    '''\n",
    "    import joblib\n",
    "    # load comments file\n",
    "    ref_df = joblib.load(f'{endpoint}_{month}{year}.joblib')\n",
    "    \n",
    "    # update DF\n",
    "    ref_df = ref_df.append(df)\n",
    "    \n",
    "    # save updated DF\n",
    "    joblib.dump(ref_df, f'{endpoint}_{month}{year}.joblib', compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pushshift(our_terms, before, after, our_filter):\n",
    "\n",
    "    '''\n",
    "    Input\n",
    "    - our_filter: our list of terms. Needs to be separated by |\n",
    "    - our_limit: number of comments to retrieve\n",
    "    - before: upper time limit. Format: int(datetime(yyyy,m,d,h,m,s).timestamp())\n",
    "    - after: lower time limit\n",
    "    Output\n",
    "    - comments_df\n",
    "    \n",
    "    Uses psaw to scrape historical data created between 'after' and 'before', \n",
    "    filtered by the list of terms in our_filter\n",
    "    '''\n",
    "    \n",
    "    from pmaw import PushshiftAPI\n",
    "    api = PushshiftAPI()\n",
    "\n",
    "    gen = api.search_comments(q=our_terms, \n",
    "                              after=after, \n",
    "                              before=before,\n",
    "                              filter=our_filter, \n",
    "                              mem_safe=True,\n",
    "                              safe_exit=True)    \n",
    "    \n",
    "    comments_df = pd.DataFrame(gen)\n",
    "\n",
    "    comments_df['created_utc'] = pd.to_datetime(comments_df.created_utc, unit='s')\n",
    "#     comments_df['created'] = pd.to_datetime(comments_df.created, unit='s')\n",
    "    comments_df['retrieved_utc'] = pd.to_datetime(comments_df.retrieved_utc, unit='s')\n",
    "\n",
    "    return comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## --- TEST ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comments_jan22.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_ids = defaultdict(list)\n",
    "joblib.dump(reference_ids, 'reference_ids.joblib', compress=3)\n",
    "\n",
    "comments_jan22 = pd.DataFrame()\n",
    "joblib.dump(comments_jan22, 'comments_jan22.joblib', compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = joblib.load('reference_ids.joblib')\n",
    "comm = joblib.load('comments_jan22.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Response cache key: 88f2ab804ecf8149b8388cb2cc21e5a3\n",
      "No previous requests to load\n",
      "223 result(s) available in Pushshift\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "day = tuple(range(1,2))\n",
    "hour = tuple(range(1))\n",
    "\n",
    "for d in day:\n",
    "    for h in hour:\n",
    "        after =  int(datetime(2022, 1, d, h, 0, 0).timestamp())\n",
    "        before = int(datetime(2022, 1, d, h, 59, 59).timestamp())\n",
    "        try:\n",
    "            df = get_pushshift(our_terms, before, after, our_filter)\n",
    "            save_unique_ids(df)\n",
    "            save_retrieved(df, 'comments', 'jan', 22)\n",
    "        except:\n",
    "            print('error')\n",
    "            break\n",
    "        print(df.shape,end='')\n",
    "\n",
    "end = time.time()\n",
    "print(' ')\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = joblib.load('reference_ids.joblib')\n",
    "print(len(ref['comment']))\n",
    "\n",
    "comm = joblib.load('comments_jan22.joblib')\n",
    "comm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since comments are retrieved soon after being posted, their features are not up to date. To update them, we use praw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get updated information on each Comment\n",
    "#### Get live data on each Submission (parent)\n",
    "#### Get live data on each User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_livedata(df):\n",
    "    '''\n",
    "    Input\n",
    "    - df: comments_df\n",
    "    Output\n",
    "    - comments_df\n",
    "    - submissions_df\n",
    "    - users_df\n",
    "    \n",
    "    Creates 2 new DFs:\n",
    "    - submissions_df\n",
    "    - users_df\n",
    "    And updates df with current information\n",
    "    \n",
    "    Iterate over df to get identifying information and \n",
    "    use it to populate the 2 new DFs\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    import praw\n",
    "    import reddit_auth\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=reddit_auth.client_id,\n",
    "        client_secret=reddit_auth.client_secret,\n",
    "        user_agent=reddit_auth.user_agent, \n",
    "        password=reddit_auth.password,\n",
    "        username=reddit_auth.username\n",
    "    )\n",
    "    \n",
    "    comments_df = df.copy()\n",
    "    submissions_data = defaultdict(list)\n",
    "    users_data = defaultdict(list)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        comment = reddit.comment(id=row.id)\n",
    "        comments_df['score'] = comment.score\n",
    "        comments_df['num_replies'] = len(list(comment.replies))\n",
    "\n",
    "        submission = reddit.submission(row.link_id[3:])\n",
    "        # if line above doens't work, use line below\n",
    "        # submission = reddit.submission(reddit.comment(id=row.id).submission.id)\n",
    "        \n",
    "        submissions_data['id'].append(submission.id)\n",
    "        submissions_data['name'].append(submission.name)\n",
    "        submissions_data['title'].append(submission.title)\n",
    "        submissions_data['num_comments'].append(submission.num_comments)\n",
    "        submissions_data['author'].append(submission.author)\n",
    "        submissions_data['created_utc'].append(datetime.fromtimestamp(submission.created_utc))\n",
    "        submissions_data['distinguished'].append(submission.distinguished)\n",
    "        submissions_data['over_18'].append(submission.over_18)\n",
    "        submissions_data['permalink'].append(submission.permalink)\n",
    "        submissions_data['score'].append(submission.score)\n",
    "        submissions_data['selftext'].append(submission.selftext)\n",
    "        submissions_data['upvote_ratio'].append(submission.upvote_ratio)\n",
    "        submissions_data['url'].append(submission.url)\n",
    "\n",
    "        user = reddit.redditor(row.author)\n",
    "        \n",
    "        users_data['comment_karma'].append(user.comment_karma)\n",
    "        users_data['created_utc'].append(datetime.fromtimestamp(user.created_utc))\n",
    "        users_data['has_verified_email'].append(user.has_verified_email)\n",
    "        users_data['is_mod'].append(user.is_mod)\n",
    "        users_data['is_gold'].append(user.is_gold)\n",
    "        users_data['link_karma'].append(user.link_karma)\n",
    "        users_data['subreddit'].append(user.subreddit)\n",
    "    \n",
    "    submissions_df = pd.DataFrame(submissions_data)\n",
    "    users_df = pd.DataFrame(users_data)\n",
    "    \n",
    "    return comments_df, submissions_df, users_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference_df = pd.DataFrame().to_csv('reference.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch: from 1/1/2022, 0h0min0sec through 1/31/2022 23h59min59sec\n",
    "\n",
    "# day = tuple(range(1,32))\n",
    "# hour = tuple(range(24))\n",
    "\n",
    "# for d in day:\n",
    "#     for h in hour:\n",
    "#         after =  int(datetime(2022, 1, d, h, m).timestamp())\n",
    "#         before = int(datetime(2022, 1, d, h, m+9).timestamp())\n",
    "#         df = get_pushshift(our_terms, our_limit, before, after, our_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Check time lapse between created_utc and retrieved_utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timedelta('0 days 00:00:15')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we may oly need to see that the max time lapse is small enough\n",
    "max(comments.retrieved_utc - comments.created_utc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we decide to get all comments for some posts:\n",
    "# below is an instance of CommentForest\n",
    "# print(reddit.submission(id=reddit.comment(id=df.id[0]).submission.id).comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if we decide to get all the comments from a user:\n",
    "# user.comments\n",
    "# user_subreddits = []\n",
    "# for comment in reddit.redditor(user.name).comments.new(limit=None):\n",
    "#     user_subreddits.append(comment.subreddit)\n",
    "# \n",
    "# Counter(user_subreddits)\n",
    "# print(len(user_subreddits)) #999, I wonder if it's cutting it off at 999..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
