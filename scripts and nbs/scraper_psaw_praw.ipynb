{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Scraping\n",
    "This notebook uses PSAW to get archive comments retrieved by Pushishift immediately after they are created. This way, we can get comments before the risk of them being deleted.\n",
    "\n",
    "Using this method, we can get older data, for instance, from 2021. **IMPORTANT**: we need to check that `created_utc` and `retrieved_utc` are close so we can ensure pushishift was retrieving posts immediately after creation at the chosen epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### --- TO DO --- \n",
    "#### before we start scraping:\n",
    "* decide on the epoch\n",
    "* finalize dictionary of terms to use in the filter for PSAW\n",
    "* decide on how to keep track of all the scraping instances. For instance, do we want to use name convention so every time the scraper makes a request and saves new DFs to Google Drive, each DF has a number in the name to reference the time or order in which it was scraped? For instance, comments_df_2022031214 or comments_df_001\n",
    "* we will probably encounter problems with ratelimits and will have to learn more about it and trouble shoot it\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install psaw\n",
    "# pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "import reddit_auth\n",
    "import praw\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import reddit_auth\n",
    "reddit = praw.Reddit(\n",
    "    client_id=reddit_auth.client_id,\n",
    "    client_secret=reddit_auth.client_secret,\n",
    "    user_agent=reddit_auth.user_agent, \n",
    "    password=reddit_auth.password,\n",
    "    username=reddit_auth.username\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get archive comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define epoch\n",
    "after = int(datetime(2022,1,1,0,0).timestamp())\n",
    "before = int(datetime(2022,1,2,0,0).timestamp())\n",
    "\n",
    "# define our filter\n",
    "our_list_of_terms = ['python','java']\n",
    "our_filter = '|'.join(our_list_of_terms)\n",
    "\n",
    "# define limit\n",
    "our_limit = 10 # so testing can be faster\n",
    "\n",
    "def get_historical_data(our_filter, our_limit, before, after):\n",
    "\n",
    "    '''\n",
    "    Input\n",
    "    - our_filter: our list of terms. Needs to be separated by |\n",
    "    - our_limit: number of comments to retrieve\n",
    "    - before: upper time limit. Format: int(datetime(yyyy,m,d,h,m,s).timestamp())\n",
    "    - after: lower time limit\n",
    "    Output\n",
    "    - comments_df\n",
    "    \n",
    "    Uses psaw to scrape historical data created between 'after' and 'before', \n",
    "    filtered by the list of terms in our_filter\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    from psaw import PushshiftAPI\n",
    "    api = PushshiftAPI()\n",
    "\n",
    "    gen = api.search_comments(q=our_filter, \n",
    "                              limit=our_limit, \n",
    "                              after=after, \n",
    "                              before=before)\n",
    "    comments_df = pd.DataFrame(gen)\n",
    "\n",
    "    comments_df.drop(labels=['d_'], inplace=True, axis=1)\n",
    "    comments_df['created_utc'] = pd.to_datetime(comments_df.created_utc, unit='s')\n",
    "    comments_df['retrieved_utc'] = pd.to_datetime(comments_df.retrieved_utc, unit='s')\n",
    "    \n",
    "    return comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since comments are retrieved soon after being posted, their features are not up to date. To update them, we use praw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get updated information on each Comment\n",
    "#### Get live data on each Submission (parent)\n",
    "#### Get live data on each User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_live_data(df):\n",
    "    '''\n",
    "    Input\n",
    "    - df: comments_df\n",
    "    Output\n",
    "    - comments_df\n",
    "    - submissions_df\n",
    "    - users_df\n",
    "    \n",
    "    Creates 2 new DFs:\n",
    "    - submissions_df\n",
    "    - users_df\n",
    "    And updates df with current information\n",
    "    \n",
    "    Iterate over df to get identifying information and \n",
    "    use it to populate the 2 new DFs\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    import praw\n",
    "    import reddit_auth\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=reddit_auth.client_id,\n",
    "        client_secret=reddit_auth.client_secret,\n",
    "        user_agent=reddit_auth.user_agent, \n",
    "        password=reddit_auth.password,\n",
    "        username=reddit_auth.username\n",
    "    )\n",
    "    \n",
    "    comments_df = df.copy()\n",
    "    submissions_df = pd.DataFrame()\n",
    "    users_df = pd.DataFrame()\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        comment = reddit.comment(id=row.id)\n",
    "        comments_df['updated_score'] = comment.score\n",
    "        comments_df['saved'] = comment.saved\n",
    "        comments_df['updated_stickied'] = comment.stickied\n",
    "        comments_df['num_replies'] = len(list(comment.replies))\n",
    "\n",
    "        submission = reddit.submission(row.link_id[3:])\n",
    "        # if line above doens't work, use line below\n",
    "        # submission = reddit.submission(reddit.comment(id=row.id).submission.id)\n",
    "        submissions_df = submissions_df.append({\n",
    "            'id': submission.id,\n",
    "            'name' : submission.name,\n",
    "            'title' : submission.title,\n",
    "            'num_comments' : submission.num_comments,\n",
    "            'author' : submission.author,\n",
    "            'created_utc' : datetime.fromtimestamp(submission.created_utc),\n",
    "            'distinguished' : submission.distinguished,\n",
    "            'is_self' : submission.is_self,\n",
    "            'link_flair_text' : submission.link_flair_text,\n",
    "            'locked' : submission.locked,\n",
    "            'over_18' : submission.over_18,\n",
    "            'permalink' : submission.permalink,\n",
    "            'saved' : submission.saved,\n",
    "            'score' : submission.score,\n",
    "            'selftext' : submission.selftext,\n",
    "            'spoiler' : submission.spoiler,\n",
    "            'stickied' : submission.stickied,\n",
    "            'upvote_ratio' : submission.upvote_ratio,\n",
    "            'url' : submission.url\n",
    "        }, ignore_index=True)\n",
    "\n",
    "        user = reddit.redditor(row.author)\n",
    "        users_df = users_df.append({\n",
    "            'id': user.id,\n",
    "            'comment_karma': user.comment_karma,\n",
    "            'created_utc': datetime.fromtimestamp(user.created_utc),\n",
    "            'has_verified_email': user.has_verified_email,\n",
    "            'is_employee': user.is_employee,\n",
    "            'is_mod': user.is_mod,\n",
    "            'is_gold': user.is_gold,\n",
    "            'link_karma': user.link_karma,\n",
    "            'subreddit': user.subreddit\n",
    "        }, ignore_index=True)\n",
    "        \n",
    "        return comments_df, submissions_df, users_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we decide to get all comments for some posts:\n",
    "# below is an instance of CommentForest\n",
    "# print(reddit.submission(id=reddit.comment(id=df.id[0]).submission.id).comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if we decide to get all the comments from a user:\n",
    "# user.comments\n",
    "# user_subreddits = []\n",
    "# for comment in reddit.redditor(user.name).comments.new(limit=None):\n",
    "#     user_subreddits.append(comment.subreddit)\n",
    "# \n",
    "# Counter(user_subreddits)\n",
    "# print(len(user_subreddits)) #999, I wonder if it's cutting it off at 999..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
